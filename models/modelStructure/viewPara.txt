SwinTransformer(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): Permute()
      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): Sequential(
      (0): SwinTransformerBlockV2(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=128, out_features=384, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=4, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=512, out_features=128, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlockV2(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=128, out_features=384, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=4, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=512, out_features=128, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): PatchMergingV2(
      (reduction): Linear(in_features=512, out_features=256, bias=False)
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (3): Sequential(
      (0): SwinTransformerBlockV2(
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=256, out_features=768, bias=True)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=8, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=1024, out_features=256, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlockV2(
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=256, out_features=768, bias=True)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=8, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=256, out_features=1024, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=1024, out_features=256, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (4): PatchMergingV2(
      (reduction): Linear(in_features=1024, out_features=512, bias=False)
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (5): Sequential(
      (0): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (2): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (3): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (4): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (5): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (6): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (7): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (8): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (9): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (10): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (11): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (12): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (13): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (14): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (15): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (16): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (17): SwinTransformerBlockV2(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=16, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=2048, out_features=512, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (6): PatchMergingV2(
      (reduction): Linear(in_features=2048, out_features=1024, bias=False)
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (7): Sequential(
      (0): SwinTransformerBlockV2(
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=1024, out_features=3072, bias=True)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=32, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=1024, out_features=4096, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=4096, out_features=1024, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlockV2(
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): ShiftedWindowAttentionV2(
          (qkv): Linear(in_features=1024, out_features=3072, bias=True)
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (cpb_mlp): Sequential(
            (0): Linear(in_features=2, out_features=512, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=512, out_features=32, bias=False)
          )
        )
        (stochastic_depth): StochasticDepth(p=0.5, mode=row)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (0): Linear(in_features=1024, out_features=4096, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=4096, out_features=1024, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (permute): Permute()
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
pth文件的类型是：<class 'collections.OrderedDict'>
pth文件的字典长度是：449
------pth文件的字典key包含------
key:
features.0.0.weight
features.0.0.bias
features.0.2.weight
features.0.2.bias
features.1.0.norm1.weight
features.1.0.norm1.bias
features.1.0.attn.logit_scale
features.1.0.attn.relative_coords_table
features.1.0.attn.relative_position_index
features.1.0.attn.qkv.weight
features.1.0.attn.qkv.bias
features.1.0.attn.proj.weight
features.1.0.attn.proj.bias
features.1.0.attn.cpb_mlp.0.weight
features.1.0.attn.cpb_mlp.0.bias
features.1.0.attn.cpb_mlp.2.weight
features.1.0.norm2.weight
features.1.0.norm2.bias
features.1.0.mlp.0.weight
features.1.0.mlp.0.bias
features.1.0.mlp.3.weight
features.1.0.mlp.3.bias
features.1.1.norm1.weight
features.1.1.norm1.bias
features.1.1.attn.logit_scale
features.1.1.attn.relative_coords_table
features.1.1.attn.relative_position_index
features.1.1.attn.qkv.weight
features.1.1.attn.qkv.bias
features.1.1.attn.proj.weight
features.1.1.attn.proj.bias
features.1.1.attn.cpb_mlp.0.weight
features.1.1.attn.cpb_mlp.0.bias
features.1.1.attn.cpb_mlp.2.weight
features.1.1.norm2.weight
features.1.1.norm2.bias
features.1.1.mlp.0.weight
features.1.1.mlp.0.bias
features.1.1.mlp.3.weight
features.1.1.mlp.3.bias
features.2.reduction.weight
features.2.norm.weight
features.2.norm.bias
features.3.0.norm1.weight
features.3.0.norm1.bias
features.3.0.attn.logit_scale
features.3.0.attn.relative_coords_table
features.3.0.attn.relative_position_index
features.3.0.attn.qkv.weight
features.3.0.attn.qkv.bias
features.3.0.attn.proj.weight
features.3.0.attn.proj.bias
features.3.0.attn.cpb_mlp.0.weight
features.3.0.attn.cpb_mlp.0.bias
features.3.0.attn.cpb_mlp.2.weight
features.3.0.norm2.weight
features.3.0.norm2.bias
features.3.0.mlp.0.weight
features.3.0.mlp.0.bias
features.3.0.mlp.3.weight
features.3.0.mlp.3.bias
features.3.1.norm1.weight
features.3.1.norm1.bias
features.3.1.attn.logit_scale
features.3.1.attn.relative_coords_table
features.3.1.attn.relative_position_index
features.3.1.attn.qkv.weight
features.3.1.attn.qkv.bias
features.3.1.attn.proj.weight
features.3.1.attn.proj.bias
features.3.1.attn.cpb_mlp.0.weight
features.3.1.attn.cpb_mlp.0.bias
features.3.1.attn.cpb_mlp.2.weight
features.3.1.norm2.weight
features.3.1.norm2.bias
features.3.1.mlp.0.weight
features.3.1.mlp.0.bias
features.3.1.mlp.3.weight
features.3.1.mlp.3.bias
features.4.reduction.weight
features.4.norm.weight
features.4.norm.bias
features.5.0.norm1.weight
features.5.0.norm1.bias
features.5.0.attn.logit_scale
features.5.0.attn.relative_coords_table
features.5.0.attn.relative_position_index
features.5.0.attn.qkv.weight
features.5.0.attn.qkv.bias
features.5.0.attn.proj.weight
features.5.0.attn.proj.bias
features.5.0.attn.cpb_mlp.0.weight
features.5.0.attn.cpb_mlp.0.bias
features.5.0.attn.cpb_mlp.2.weight
features.5.0.norm2.weight
features.5.0.norm2.bias
features.5.0.mlp.0.weight
features.5.0.mlp.0.bias
features.5.0.mlp.3.weight
features.5.0.mlp.3.bias
features.5.1.norm1.weight
features.5.1.norm1.bias
features.5.1.attn.logit_scale
features.5.1.attn.relative_coords_table
features.5.1.attn.relative_position_index
features.5.1.attn.qkv.weight
features.5.1.attn.qkv.bias
features.5.1.attn.proj.weight
features.5.1.attn.proj.bias
features.5.1.attn.cpb_mlp.0.weight
features.5.1.attn.cpb_mlp.0.bias
features.5.1.attn.cpb_mlp.2.weight
features.5.1.norm2.weight
features.5.1.norm2.bias
features.5.1.mlp.0.weight
features.5.1.mlp.0.bias
features.5.1.mlp.3.weight
features.5.1.mlp.3.bias
features.5.2.norm1.weight
features.5.2.norm1.bias
features.5.2.attn.logit_scale
features.5.2.attn.relative_coords_table
features.5.2.attn.relative_position_index
features.5.2.attn.qkv.weight
features.5.2.attn.qkv.bias
features.5.2.attn.proj.weight
features.5.2.attn.proj.bias
features.5.2.attn.cpb_mlp.0.weight
features.5.2.attn.cpb_mlp.0.bias
features.5.2.attn.cpb_mlp.2.weight
features.5.2.norm2.weight
features.5.2.norm2.bias
features.5.2.mlp.0.weight
features.5.2.mlp.0.bias
features.5.2.mlp.3.weight
features.5.2.mlp.3.bias
features.5.3.norm1.weight
features.5.3.norm1.bias
features.5.3.attn.logit_scale
features.5.3.attn.relative_coords_table
features.5.3.attn.relative_position_index
features.5.3.attn.qkv.weight
features.5.3.attn.qkv.bias
features.5.3.attn.proj.weight
features.5.3.attn.proj.bias
features.5.3.attn.cpb_mlp.0.weight
features.5.3.attn.cpb_mlp.0.bias
features.5.3.attn.cpb_mlp.2.weight
features.5.3.norm2.weight
features.5.3.norm2.bias
features.5.3.mlp.0.weight
features.5.3.mlp.0.bias
features.5.3.mlp.3.weight
features.5.3.mlp.3.bias
features.5.4.norm1.weight
features.5.4.norm1.bias
features.5.4.attn.logit_scale
features.5.4.attn.relative_coords_table
features.5.4.attn.relative_position_index
features.5.4.attn.qkv.weight
features.5.4.attn.qkv.bias
features.5.4.attn.proj.weight
features.5.4.attn.proj.bias
features.5.4.attn.cpb_mlp.0.weight
features.5.4.attn.cpb_mlp.0.bias
features.5.4.attn.cpb_mlp.2.weight
features.5.4.norm2.weight
features.5.4.norm2.bias
features.5.4.mlp.0.weight
features.5.4.mlp.0.bias
features.5.4.mlp.3.weight
features.5.4.mlp.3.bias
features.5.5.norm1.weight
features.5.5.norm1.bias
features.5.5.attn.logit_scale
features.5.5.attn.relative_coords_table
features.5.5.attn.relative_position_index
features.5.5.attn.qkv.weight
features.5.5.attn.qkv.bias
features.5.5.attn.proj.weight
features.5.5.attn.proj.bias
features.5.5.attn.cpb_mlp.0.weight
features.5.5.attn.cpb_mlp.0.bias
features.5.5.attn.cpb_mlp.2.weight
features.5.5.norm2.weight
features.5.5.norm2.bias
features.5.5.mlp.0.weight
features.5.5.mlp.0.bias
features.5.5.mlp.3.weight
features.5.5.mlp.3.bias
features.5.6.norm1.weight
features.5.6.norm1.bias
features.5.6.attn.logit_scale
features.5.6.attn.relative_coords_table
features.5.6.attn.relative_position_index
features.5.6.attn.qkv.weight
features.5.6.attn.qkv.bias
features.5.6.attn.proj.weight
features.5.6.attn.proj.bias
features.5.6.attn.cpb_mlp.0.weight
features.5.6.attn.cpb_mlp.0.bias
features.5.6.attn.cpb_mlp.2.weight
features.5.6.norm2.weight
features.5.6.norm2.bias
features.5.6.mlp.0.weight
features.5.6.mlp.0.bias
features.5.6.mlp.3.weight
features.5.6.mlp.3.bias
features.5.7.norm1.weight
features.5.7.norm1.bias
features.5.7.attn.logit_scale
features.5.7.attn.relative_coords_table
features.5.7.attn.relative_position_index
features.5.7.attn.qkv.weight
features.5.7.attn.qkv.bias
features.5.7.attn.proj.weight
features.5.7.attn.proj.bias
features.5.7.attn.cpb_mlp.0.weight
features.5.7.attn.cpb_mlp.0.bias
features.5.7.attn.cpb_mlp.2.weight
features.5.7.norm2.weight
features.5.7.norm2.bias
features.5.7.mlp.0.weight
features.5.7.mlp.0.bias
features.5.7.mlp.3.weight
features.5.7.mlp.3.bias
features.5.8.norm1.weight
features.5.8.norm1.bias
features.5.8.attn.logit_scale
features.5.8.attn.relative_coords_table
features.5.8.attn.relative_position_index
features.5.8.attn.qkv.weight
features.5.8.attn.qkv.bias
features.5.8.attn.proj.weight
features.5.8.attn.proj.bias
features.5.8.attn.cpb_mlp.0.weight
features.5.8.attn.cpb_mlp.0.bias
features.5.8.attn.cpb_mlp.2.weight
features.5.8.norm2.weight
features.5.8.norm2.bias
features.5.8.mlp.0.weight
features.5.8.mlp.0.bias
features.5.8.mlp.3.weight
features.5.8.mlp.3.bias
features.5.9.norm1.weight
features.5.9.norm1.bias
features.5.9.attn.logit_scale
features.5.9.attn.relative_coords_table
features.5.9.attn.relative_position_index
features.5.9.attn.qkv.weight
features.5.9.attn.qkv.bias
features.5.9.attn.proj.weight
features.5.9.attn.proj.bias
features.5.9.attn.cpb_mlp.0.weight
features.5.9.attn.cpb_mlp.0.bias
features.5.9.attn.cpb_mlp.2.weight
features.5.9.norm2.weight
features.5.9.norm2.bias
features.5.9.mlp.0.weight
features.5.9.mlp.0.bias
features.5.9.mlp.3.weight
features.5.9.mlp.3.bias
features.5.10.norm1.weight
features.5.10.norm1.bias
features.5.10.attn.logit_scale
features.5.10.attn.relative_coords_table
features.5.10.attn.relative_position_index
features.5.10.attn.qkv.weight
features.5.10.attn.qkv.bias
features.5.10.attn.proj.weight
features.5.10.attn.proj.bias
features.5.10.attn.cpb_mlp.0.weight
features.5.10.attn.cpb_mlp.0.bias
features.5.10.attn.cpb_mlp.2.weight
features.5.10.norm2.weight
features.5.10.norm2.bias
features.5.10.mlp.0.weight
features.5.10.mlp.0.bias
features.5.10.mlp.3.weight
features.5.10.mlp.3.bias
features.5.11.norm1.weight
features.5.11.norm1.bias
features.5.11.attn.logit_scale
features.5.11.attn.relative_coords_table
features.5.11.attn.relative_position_index
features.5.11.attn.qkv.weight
features.5.11.attn.qkv.bias
features.5.11.attn.proj.weight
features.5.11.attn.proj.bias
features.5.11.attn.cpb_mlp.0.weight
features.5.11.attn.cpb_mlp.0.bias
features.5.11.attn.cpb_mlp.2.weight
features.5.11.norm2.weight
features.5.11.norm2.bias
features.5.11.mlp.0.weight
features.5.11.mlp.0.bias
features.5.11.mlp.3.weight
features.5.11.mlp.3.bias
features.5.12.norm1.weight
features.5.12.norm1.bias
features.5.12.attn.logit_scale
features.5.12.attn.relative_coords_table
features.5.12.attn.relative_position_index
features.5.12.attn.qkv.weight
features.5.12.attn.qkv.bias
features.5.12.attn.proj.weight
features.5.12.attn.proj.bias
features.5.12.attn.cpb_mlp.0.weight
features.5.12.attn.cpb_mlp.0.bias
features.5.12.attn.cpb_mlp.2.weight
features.5.12.norm2.weight
features.5.12.norm2.bias
features.5.12.mlp.0.weight
features.5.12.mlp.0.bias
features.5.12.mlp.3.weight
features.5.12.mlp.3.bias
features.5.13.norm1.weight
features.5.13.norm1.bias
features.5.13.attn.logit_scale
features.5.13.attn.relative_coords_table
features.5.13.attn.relative_position_index
features.5.13.attn.qkv.weight
features.5.13.attn.qkv.bias
features.5.13.attn.proj.weight
features.5.13.attn.proj.bias
features.5.13.attn.cpb_mlp.0.weight
features.5.13.attn.cpb_mlp.0.bias
features.5.13.attn.cpb_mlp.2.weight
features.5.13.norm2.weight
features.5.13.norm2.bias
features.5.13.mlp.0.weight
features.5.13.mlp.0.bias
features.5.13.mlp.3.weight
features.5.13.mlp.3.bias
features.5.14.norm1.weight
features.5.14.norm1.bias
features.5.14.attn.logit_scale
features.5.14.attn.relative_coords_table
features.5.14.attn.relative_position_index
features.5.14.attn.qkv.weight
features.5.14.attn.qkv.bias
features.5.14.attn.proj.weight
features.5.14.attn.proj.bias
features.5.14.attn.cpb_mlp.0.weight
features.5.14.attn.cpb_mlp.0.bias
features.5.14.attn.cpb_mlp.2.weight
features.5.14.norm2.weight
features.5.14.norm2.bias
features.5.14.mlp.0.weight
features.5.14.mlp.0.bias
features.5.14.mlp.3.weight
features.5.14.mlp.3.bias
features.5.15.norm1.weight
features.5.15.norm1.bias
features.5.15.attn.logit_scale
features.5.15.attn.relative_coords_table
features.5.15.attn.relative_position_index
features.5.15.attn.qkv.weight
features.5.15.attn.qkv.bias
features.5.15.attn.proj.weight
features.5.15.attn.proj.bias
features.5.15.attn.cpb_mlp.0.weight
features.5.15.attn.cpb_mlp.0.bias
features.5.15.attn.cpb_mlp.2.weight
features.5.15.norm2.weight
features.5.15.norm2.bias
features.5.15.mlp.0.weight
features.5.15.mlp.0.bias
features.5.15.mlp.3.weight
features.5.15.mlp.3.bias
features.5.16.norm1.weight
features.5.16.norm1.bias
features.5.16.attn.logit_scale
features.5.16.attn.relative_coords_table
features.5.16.attn.relative_position_index
features.5.16.attn.qkv.weight
features.5.16.attn.qkv.bias
features.5.16.attn.proj.weight
features.5.16.attn.proj.bias
features.5.16.attn.cpb_mlp.0.weight
features.5.16.attn.cpb_mlp.0.bias
features.5.16.attn.cpb_mlp.2.weight
features.5.16.norm2.weight
features.5.16.norm2.bias
features.5.16.mlp.0.weight
features.5.16.mlp.0.bias
features.5.16.mlp.3.weight
features.5.16.mlp.3.bias
features.5.17.norm1.weight
features.5.17.norm1.bias
features.5.17.attn.logit_scale
features.5.17.attn.relative_coords_table
features.5.17.attn.relative_position_index
features.5.17.attn.qkv.weight
features.5.17.attn.qkv.bias
features.5.17.attn.proj.weight
features.5.17.attn.proj.bias
features.5.17.attn.cpb_mlp.0.weight
features.5.17.attn.cpb_mlp.0.bias
features.5.17.attn.cpb_mlp.2.weight
features.5.17.norm2.weight
features.5.17.norm2.bias
features.5.17.mlp.0.weight
features.5.17.mlp.0.bias
features.5.17.mlp.3.weight
features.5.17.mlp.3.bias
features.6.reduction.weight
features.6.norm.weight
features.6.norm.bias
features.7.0.norm1.weight
features.7.0.norm1.bias
features.7.0.attn.logit_scale
features.7.0.attn.relative_coords_table
features.7.0.attn.relative_position_index
features.7.0.attn.qkv.weight
features.7.0.attn.qkv.bias
features.7.0.attn.proj.weight
features.7.0.attn.proj.bias
features.7.0.attn.cpb_mlp.0.weight
features.7.0.attn.cpb_mlp.0.bias
features.7.0.attn.cpb_mlp.2.weight
features.7.0.norm2.weight
features.7.0.norm2.bias
features.7.0.mlp.0.weight
features.7.0.mlp.0.bias
features.7.0.mlp.3.weight
features.7.0.mlp.3.bias
features.7.1.norm1.weight
features.7.1.norm1.bias
features.7.1.attn.logit_scale
features.7.1.attn.relative_coords_table
features.7.1.attn.relative_position_index
features.7.1.attn.qkv.weight
features.7.1.attn.qkv.bias
features.7.1.attn.proj.weight
features.7.1.attn.proj.bias
features.7.1.attn.cpb_mlp.0.weight
features.7.1.attn.cpb_mlp.0.bias
features.7.1.attn.cpb_mlp.2.weight
features.7.1.norm2.weight
features.7.1.norm2.bias
features.7.1.mlp.0.weight
features.7.1.mlp.0.bias
features.7.1.mlp.3.weight
features.7.1.mlp.3.bias
norm.weight
norm.bias
head.weight
head.bias
------------------------------
